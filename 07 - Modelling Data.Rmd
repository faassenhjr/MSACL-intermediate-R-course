---
title: 'Lesson 7: Method Comparison & Fitting Models to the Data'
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(modelr)
library(magrittr)
library(lubridate)
```

## Overview of data

A common activity in the clinical laboratory and in translation of an assay into the clinic is performing a method comparison of a new method with a previous method. In this lesson we will explore data from two methods for a human chorionic gonadotropin (hCG), which in this case is part of a panel to assess pregnant women for a fetus with trisomy 21 (Down's syndrome) or a neural tube defect. This data has values for a method A and a method B in a simple spreadsheet, with sheets corresponding to each of the assays in the panel (alpha fetoprotein and unconjugated estriol being the other two tests).

First we start by loading in hCG data from the "MS HCG" sheet.

```{r}
hcg <- read_excel(path="data/method_validation_data.xlsx", 
                sheet="MS HCG")
glimpse(hcg)
```

Let's use pipes to summarize and calculate a few statistics:

```{r}
hcg %>%
  summarize(method_a_mean = mean(method_a), 
            method_a_sd = sd(method_a),
            method_b_mean = mean(method_b), 
            method_b_sd = sd(method_b))
```

## Overlapping histograms

What if we want to plot the distribution of both `method_a` and `method_b` in the same plot? We've used `geom_histogram` previously. Let's try the related ggplot function `geom_freqpoly`, starting with a single method.

```{r}
ggplot(data = hcg) + 
  geom_freqpoly(bins=20, 
                aes(x=method_a))
```

Now, let's add a second method and mark the two using `geom_freqpoly`.

```{r}
ggplot(data = hcg) + 
  geom_freqpoly(bins=20, aes(x=method_a, color="blue")) +
  geom_freqpoly(bins=20, aes(x=method_b, color="red")) 
```

**Exercise 1:**

Make a similar display of method a and method b distributions using the `geom_density` function. Set the `fill` and `color` functions to distinguish between the two methods. Test using the `alpha` parameter to increase the shape translucency

```{r, eval = FALSE}
ggplot(data = ) + 
  geom_density(aes(x =, fill = , color = ), alpha = ) +
  geom_density(aes(x = , fill = , color= ), alpha = )
```

**End exercise**

### Reorganizing a tibble to facilitate plotting

These plots require making separate calls to the `geom_*` functions for each method. This works, but makes legends needing manual cleanup and does not scale well.

Instead, another way is to wrangle the data for a plot is to create a "long dataframe" using the `gather` function. Recall that this "unpacks" multiple columns into just two columns, where the **first column** is the `key` and the **second column** is the `value`:
[assets/data_gather.png]()

```{r}
long_hcg <- hcg %>%
  gather(key="method", value="value", 
         -specimen)
head(long_hcg)
```

Take a moment to compare the `hcg` and `long_hcg` objects. How are they different? Note that when we have a "long" datafarme, every row is a named observation (also known as a key-value pair). For reference, the reverse transformation (from "long" to "wide") is done with the `spread` function.

Once in a long-form format, we can tell ggplot to use both the `value` variable for our x-axis and the `method` variable to set the coloring (fill and color) and the resulting legend.

```{r}
ggplot(long_hcg) + 
  geom_density(aes(x=value, fill=method, color=method), 
               alpha=0.5)
```

## Method comparison (t-tests, and more)

### Using a statistical test

R is a statistical programming language, so simple statistical testing is straightforward:

```{r}
# Note we are using the paired=TRUE variant of the t.test, since we have paired measurements.
t.test(hcg$method_a, hcg$method_b, 
       paired=TRUE)
```

For more information on the `t.test` function, (follow this link)[https://www.statmethods.net/stats/ttest.html].

**Exercise 2:**
Evaluate parametric comparability of method means after log-transformation

```{r, eval = FALSE}

```

**End exercise**

### Using the RIGHT statistical test
Is `t.test` the right function? Consider the histograms above and our previous work with log normalizing the values. 

|Populations|Parametric|Non-parametric|
|:-------------|:-------------------------:|:-------------------------:|
|Two populations|t-test|Mann-Whitney U|
|Many populations|ANOVA|Kruskal Wallis / one-way anova|
|Populations across several treatments/times|repeated measures ANOVA|Friedman test|

**Exercise 3:** 
Using the table above, select the _right_ test for comparing `method_a` and `method_b`. Look up the function call using google, R documentation or any other source. Write out the function and calculate a p-value below

```{r, eval = FALSE}

```

**End Exercise** 

## Regression

### Simple linear regression

Let's begin by simply plotting `method_a` and `method_b` as a scatter plot. Notice how we are using the `aes()` to define "mappings" from our data to the x and y coordinates:

```{r}
ggplot(hcg) +
  geom_point(aes(x = method_a, y = method_b))
```

Adding a least-squares regression line is easy with a little bit of magic from `ggplot`. The `lm` (Linear Model) function does all the work here!

```{r}
ggplot(hcg) +
  geom_point(aes(x = method_a, y = method_b)) +
  geom_smooth(method = "lm", aes(x = method_a, y = method_b))
```

It's tough to see the fit in the low result range, so we can transform our axis:

```{r}
ggplot(hcg) +
  geom_point(aes(x = method_a, y = method_b)) +
  geom_smooth(method = "lm", aes(x = method_a, y = method_b)) +
  scale_x_log10() + scale_y_log10()
```

What if we want to just extract the coefficients of the linear model? We can utilize R's formula notation format and the `lm` function:

```{r}
regression <- lm(method_b ~ method_a, hcg)
summary(regression)
coef(regression)
```

## Deming regression

In fact, a least-squares regression, while a good approximation for this type of data, assumes that the x-dimension has no measurement error so it minimizes errors only in the y-dimension. The *Deming regression* differs from the simple linear regression in that it accounts for errors in observations on both the x- and the y- axes, thus making it more suitable for estimating a best-fit line between two measured variables.

The Deming regression and other similar regressions are not built into the standard linear models function in R. Fortunately, there is a package written by staff at Roche called [mcr](https://cran.r-project.org/web/packages/mcr/mcr.pdf) to perform some of these bread and butter clinical laboratory regression methods. One issue with the package is that the output is not straightforward to work with. Many of the mcr functions produce a complex object that does not fit a standard tidy framework. The useful values can be extracted by digging more deeply into the output.

A Deming regression of methods A and B appears below. To pull the quantitative data we care out of the model, we use the `@` character and call the specific component of the object we care about (in this care "para").

```{r}
#install.packages("mcr", dependencies = TRUE)
library(mcr)            # remember, you may need to install.packages("mcr") in the console first!
deming_results <- mcreg(hcg$method_a, hcg$method_b, 
                        method.reg = "Deming")
deming_results@para         # "para" short for "parameters"
# this is a library/method specific term here
```

To see what slots (attributes) `deming_results` has beyond `para`, look at `glimpse(deming_results)`

**Exercise 4:**

Another issue in standard regression is if the random error scales with analyte concentration, then the observed absolute errors are higher for the higher concentrations. This means that error at the high-end is more heavily factored into the regression fit. One way of adjusting for this is weighting errors for high concentrations less. 

Run the regression using the weighted deming method of the `mcreg` function. How do the slope and intercept differ?

```{r, eval = FALSE}
wdeming_results <-
  
wdeming_results@para
```

**End exercise**

Now let's add the lines to our plot. We can use the `geom_abline()` ggplot function to add a line with a slope and intercept. The intercept and slope are stored in `deming_results@para[1]` and `deming_results@para[2]`, respectively.

```{r}
ggplot(hcg) +
  geom_point(aes(x = method_a, y = method_b)) +
  geom_smooth(method = "lm", aes(x = method_a, y = method_b), se = FALSE) +
  geom_abline(
    intercept = deming_results@para[1], slope = deming_results@para[2],
    color = "red"
  )
```

What about the weighted deming fit line? What about the subrange between 0 and 40000? Let's also add a 1:1 line to help interpret fit.

```{r}
wdeming_results <- mcreg(hcg$method_a, hcg$method_b, 
                        method.reg = "WDeming")
ggplot(hcg) +
  geom_point(aes(x=method_a, y=method_b))  +
  geom_smooth(method = "lm", aes(x=method_a, y=method_b), se=FALSE) +   #blue
  geom_abline(intercept = deming_results@para[1], slope = deming_results@para[2], 
              color="red") +
  geom_abline(intercept = wdeming_results@para[1], slope = wdeming_results@para[2], 
              color="yellow") +
  xlim(0, 40000) + ylim(0, 40000) +
  geom_abline(intercept=0, slope=1, linetype=2, color="gray")
```


## Passing-Bablock

Rather than performing a Deming, we may want to perform a Passing-Bablock regression instead. What is the distinct between Deming and Passing-Bablok? At a high level, the Deming regression assumes that errors are distributed normally, while Passing-Bablok is non-parametric and does not make this assumption.

We extract the useful model parameters similarly to the Deming regression:

```{r}
PB_results <- mcreg(hcg$method_a, hcg$method_b, method.reg = "PaBa")
PB_results@para
```

**Exercise 5:**
Add another `geom_abline` to the plot above for the Passing-Bablock regression coefficients determined above.

```{r, eval = FALSE}
ggplot(hcg) +
  geom_point()  +
  geom_smooth() +  #blue
  geom_abline() +
  xlim(0, 40000) + ylim(0, 40000) +
  geom_abline() +
  geom_abline()
```

**End Exercise**

## Compare methods by concordance relative to decision thresholds

Next, let's compare method A and B using decision thresholds. For the purpose of this tutorial, we will simply use 25,000 as our threshold.

```{r}
threshold <- 25000

tmp <- hcg %>%
  mutate(method_a_pos = method_a > threshold,   # Create binary indicator for method_a
         method_b_pos = method_b > threshold)
  table(x=tmp$method_a_pos, y=tmp$method_b_pos)
```
Looking at this table, method_a and method_b are *discordant* across our threshold in 40 cases, and *concordant* in 58 + 48 cases.

A tidy way to do this without using `tmp` and `table` is:

```{r}
threshold <- 25000

hcg %>%
  mutate(method_a_pos = method_a > threshold,   # Create binary indicator for method_a
         method_b_pos = method_b > threshold) %>%
  count(method_a_pos, method_b_pos)
```

Now to convert this into a standard concordance table:

```{r}
hcg %>%
  mutate(method_a_pos = method_a > threshold,   # Create binary indicator for method_a
         method_b_pos = method_b > threshold) %>%
  count(method_a_pos, method_b_pos)  %>%
  spread(method_b_pos, n, fill=0, sep=".")   # Spreads method_b_pos from a single variable 
  # to a variable for each value
```


**Exercise 6**

Write code to compare accuracy across two different decision thresholds (25000 and 50000, for example)

_Hint #1_: In the `mutate` function, use the `case_when` function to break a numerical range into multiple a set of factor levels (categories).

_Hint #2_: Look at previous code for inspiration!

```{r, eval = FALSE}
hcg %>%
  mutate(method_a_bin = case_when(
     ~ ,
     ~ ,
     ~ )) %>%
  mutate(method_b_bin = case_when(
    ~ ,
     ~ ,
     ~ 
  )) %>%
    %>%
   
```

**End Exercise** 

## Acknowledgement

This lesson was adapted from Daniel Hermann's lesson on method comparison that was taught in the [AACC Basic R Course](https://github.com/pcmathias/AACC-Introduction-to-R).

## Summary

- When comparing methods, a useful first step is a comparison of summary statistics and distributions
- Performing a statistical test to assess whether the distributions from two methods are likely to have originated from the same distribution is helpful, and the test choice should be informed by the types of distributions
- Built-in models are available for the most common types of fits in ggplot2
- However, additional complexity is needed to overlay more sophisticated models such as Deming regression and Passing Bablok
  - Data from more complex models can be extracted and used to manually generate lines of fit

